{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "#### EE-556 Mathematics of Data - Fall 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework covers Lectures 8-12. Please take a look at the material for the context and notation.\n",
    "\n",
    "In this homework we will study minimax problems. We will begin with some theoretical analysis and in a second part you will implement a Wasserstein Generative Adversarial Network (WGAN). \n",
    "\n",
    "These notebooks should expose you to the fundamentals of GAN training at a basic level, as well as some of the theory behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Minimax problems - 65 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Theoretical recap: stationary points and convergence in minmax games - 25 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a stylized function $f:\\mathbb{R}^4 \\to \\mathbb{R}$ with variables $x_1,x_2,y_1,y_2$, we denote by\n",
    "$x=(x_1,x_2) \\in \\mathbb{R}^2$, $y=(y_1,y_2) \\in \\mathbb{R}^2$, the function has the form of\n",
    "$\n",
    "f(x,y) = (a x - b)^\\top (a y - c), \\quad a \\neq 0, \\; b,c \\in \\mathbb{R}^2.\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(1)__ (5 points) Write down the first-order stationary points of $f$, and\n",
    "        classify them as local minimum, local maximum, or saddle point by\n",
    "        inspecting its Hessian.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall the definition of first-order stationary points:\n",
    "\n",
    "A point $\\bar{x}$ is a first-order stationary point of a twice differentiable function $f$ if  \n",
    "\n",
    "$$\\nabla f(\\bar{x}) = 0.$$\n",
    "\n",
    "Also, recall the following:\n",
    "\n",
    "Let $\\bar{x}$ be a stationary point of a twice differentiable function $f$.\n",
    "\n",
    "1. If $\\nabla^2 f(\\bar{x}) \\succ 0$, then the point $\\bar{x}$ is called a local minimum or a second order stationary point (SOSP).\n",
    "2. If $\\nabla^2 f(\\bar{x}) \\prec 0$, then the point $\\bar{x}$ is called a local maximum.\n",
    "3. If $\\nabla^2 f(\\bar{x}) = 0$, then the point $\\bar{x}$ can be a saddle point, a local minimum, or a local maximum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x,y) = (ax - b)^T (ay - c)\n",
    "= \\begin{bmatrix} a x_1 - b_1 & a x_2 - b_2 \\end{bmatrix}\n",
    "  \\begin{bmatrix} a y_1 - c_1 \\\\ a y_2 - c_2 \\end{bmatrix}=$\n",
    "\n",
    "$= (a x_1 - b_1)(a y_1 - c_1) + (a x_2 - b_2)(a y_2 - c_2)=$\n",
    "\n",
    "$= a^2 x_1 y_1 - a x_1 c_1 - a b_1 y_1 + b_1 c_1\n",
    "  + a^2 x_2 y_2 - a x_2 c_2 - a b_2 y_2 + b_2 c_2$\n",
    "\n",
    "$\\dfrac{\\partial f}{\\partial x_1} = a^2 y_1 - a c_1$\n",
    "\n",
    "$\\dfrac{\\partial f}{\\partial x_2} = a^2 y_2 - a c_2$\n",
    "\n",
    "$\\nabla_x f(x,y) =\n",
    "\\begin{pmatrix}\n",
    "a^2 y_1 - a c_1 \\\\\n",
    "a^2 y_2 - a c_2\n",
    "\\end{pmatrix}$\n",
    "\n",
    "$\\dfrac{\\partial f}{\\partial y_1} = a^2 x_1 - a b_1$\n",
    "\n",
    "$\\dfrac{\\partial f}{\\partial y_2} = a^2 x_2 - a b_2$\n",
    "\n",
    "$\\nabla_y f(x,y) =\n",
    "\\begin{pmatrix}\n",
    "a^2 x_1 - a b_1 \\\\\n",
    "a^2 x_2 - a b_2\n",
    "\\end{pmatrix}$\n",
    "\n",
    "\n",
    "For the derivative we have\n",
    "\n",
    "$$\n",
    "\\nabla f(x,y)\n",
    "=\n",
    "\\big( \\nabla_x f(x,y),\\ \\nabla_y f(x,y) \\big)\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "a^2 y_1 - a c_1 & a^2 x_1 - a b_1 \\\\\n",
    "a^2 y_2 - a c_2 & a^2 x_2 - a b_2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\big( a^2 y - a c,\\ \\ a^2 x - a b \\big)\n",
    "$$\n",
    "\n",
    "\n",
    "To find the stationary points we need to calculate $\\nabla f(x,y) = 0$.\n",
    "\n",
    "This means that\n",
    "\n",
    "$$\n",
    "a^2 y - a c = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^2 x - a b = 0\n",
    "$$\n",
    "\n",
    "By solving this we get\n",
    "\n",
    "$$\n",
    "y^\\star = \\frac{1}{a} c\n",
    "\\qquad\\text{and}\\qquad\n",
    "x^\\star = \\frac{1}{a} b\n",
    "$$\n",
    "\n",
    "So the first order stationary point is \n",
    "\n",
    "$$\n",
    "(x^\\star, y^\\star) = \\left( \\frac{b}{a}, \\frac{c}{a} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to evaluate the Hessian. We start fromt the block $H_{xy}$.\n",
    "\n",
    "Recall: \n",
    "\n",
    "$\\nabla_x f(x,y) =\n",
    "\\begin{pmatrix}\n",
    "a^2 y_1 - a c_1 \\\\\n",
    "a^2 y_2 - a c_2\n",
    "\\end{pmatrix}$\n",
    "\n",
    "Now we need to differentiate this with respect to $y=(y_1,y_2)$\n",
    "\n",
    "$\\dfrac{\\partial^2 f}{\\partial x_1 \\partial y_1}\n",
    "= \\dfrac{\\partial}{\\partial y_1}(a^2 y_1 - a c_1) = a^2$\n",
    "\n",
    "$\\dfrac{\\partial^2 f}{\\partial x_1 \\partial y_2}\n",
    "= \\dfrac{\\partial}{\\partial y_2}(a^2 y_1 - a c_1) = 0$\n",
    "\n",
    "$\\dfrac{\\partial^2 f}{\\partial x_2 \\partial y_1}\n",
    "= \\dfrac{\\partial}{\\partial y_1}(a^2 y_2 - a c_2) = 0$\n",
    "\n",
    "$\\dfrac{\\partial^2 f}{\\partial x_2 \\partial y_2}\n",
    "= \\dfrac{\\partial}{\\partial y_2}(a^2 y_2 - a c_2) = a^2$\n",
    "\n",
    "$H_{xy} =\n",
    "\\begin{bmatrix}\n",
    "a^2 & 0 \\\\\n",
    "0   & a^2\n",
    "\\end{bmatrix}\n",
    "= a^2 I_2$\n",
    "\n",
    "The same we apply to get $H_{yx}$.\n",
    "\n",
    "$\\nabla_y f(x,y) =\n",
    "\\begin{pmatrix}\n",
    "a^2 x_1 - a b_1 \\\\\n",
    "a^2 x_2 - a b_2\n",
    "\\end{pmatrix}$\n",
    "\n",
    "$\\dfrac{\\partial^2 f}{\\partial y_1 \\partial x_1}\n",
    "= \\dfrac{\\partial}{\\partial x_1}(a^2 x_1 - a b_1) = a^2$\n",
    "\n",
    "$\\dfrac{\\partial^2 f}{\\partial y_1 \\partial x_2}\n",
    "= \\dfrac{\\partial}{\\partial x_2}(a^2 x_1 - a b_1) = 0$\n",
    "\n",
    "$\\dfrac{\\partial^2 f}{\\partial y_2 \\partial x_1}\n",
    "= \\dfrac{\\partial}{\\partial x_1}(a^2 x_2 - a b_2) = 0$\n",
    "\n",
    "$\\dfrac{\\partial^2 f}{\\partial y_2 \\partial x_2}\n",
    "= \\dfrac{\\partial}{\\partial x_2}(a^2 x_2 - a b_2) = a^2$\n",
    "\n",
    "So we have\n",
    "$H_{yx} =\n",
    "\\begin{bmatrix}\n",
    "a^2 & 0 \\\\\n",
    "0   & a^2\n",
    "\\end{bmatrix}\n",
    "= a^2 I_2$\n",
    "\n",
    "As $\\nabla_x f(x,y)$ does not depend on $x$ we have\n",
    "\n",
    "$H_{xx} =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}$\n",
    "\n",
    "The same for $\\nabla_y f(x,y)$. It does not depend on $y$ so\n",
    "\n",
    "$H_{yy} =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Combining these blocks in Hessian we have\n",
    "\n",
    "$H =\n",
    "\\begin{bmatrix}\n",
    "0_2      & a^2 I_2 \\\\\n",
    "a^2 I_2  & 0_2\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to check whether $H$ is positive /\n",
    "negative definite or indefinite.\n",
    "\n",
    "We take generic vector $z = (x, y) \\in \\mathbb{R}^2$\n",
    "\n",
    "$$\n",
    "z^T H z =\n",
    "\\begin{bmatrix} x^T & y^T \\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0 & a^2 I_2 \\\\\n",
    "a^2 I_2 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} x \\\\ y \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\begin{bmatrix} x_1 & x_2 & y_1 & y_2 \\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & a^2 & 0 \\\\\n",
    "0 & 0 & 0 & a^2 \\\\\n",
    "a^2 & 0 & 0 & 0 \\\\\n",
    "0 & a^2 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\\\ y_1 \\\\ y_2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= a^2(x_1 y_1 + x_2 y_2) + a^2(x_1 y_1 + x_2 y_2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 2 a^2 (x_1 y_1 + x_2 y_2) = 2 a^2 x^T y\n",
    "$$\n",
    "\n",
    "For $x = y \\neq 0$\n",
    "\n",
    "$$\n",
    "z^T H z = 2 a^2 x^T x = 2 a^2 |x|^2 > 0\n",
    "$$\n",
    "\n",
    "For $x = -y$\n",
    "\n",
    "$$\n",
    "z^T H z = 2 a^2 x^T (-x) = - 2 a^2 |x|^2 < 0\n",
    "$$\n",
    "\n",
    "From this we can conclude that the matrix\n",
    "is neither positive nor negative definite as it\n",
    "gives both positive and negative values depending on $x$.\n",
    "\n",
    "Therefore we can conclude that the stationary point $(x^\\star, y^\\star) = \\left( \\frac{b}{a}, \\frac{c}{a} \\right)$ is a **saddle point**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(2)__ (5 points) Find the solution $(x^\\star,y^\\star)$ to the\n",
    "        minimax problem $\\min_x \\max_y f(x, y)$. You can quantify the solution\n",
    "        by using the following saddle point inequality:  $f(x^\\star, y^\\star)\n",
    "        \\geq f(x^\\star, y)$ and $f(x^\\star, y^\\star) \\leq f(x, y^\\star)$, for\n",
    "        all $x, y$.\n",
    "        \n",
    "**HINT:** $(x^\\star, y^\\star)$ can only be one of the critical points you found in (1), just evaluate $f$ at every place in the inequalities to check the optimality!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us evaluate the value of $f(x,y)$ in the saddle point.\n",
    "\n",
    "$$\n",
    "f(x^, y^) = f\\left( \\tfrac{b}{a}, \\tfrac{c}{a} \\right)\n",
    "= (a \\tfrac{b}{a} - b)^T (a \\tfrac{c}{a} - c) = 0\n",
    "$$\n",
    "\n",
    "Now we can check $f(x^*, y)$ for any $y$.\n",
    "\n",
    "$$\n",
    "f(x^*, y) = f\\left( \\tfrac{b}{a}, y \\right)\n",
    "= (a \\tfrac{b}{a} - b)^T (a y - c) = 0\n",
    "$$\n",
    "\n",
    "Therefore we have that\n",
    "\n",
    "$$\n",
    "f(x^, y^) \\ge f(x^*, y) \\qquad \\forall\\, y\n",
    "$$\n",
    "\n",
    "as $0 = 0$.\n",
    "\n",
    "So at $x^\\star$ every $y$ gives the same value of $0$ and we can see that $y^\\star$ is\n",
    "indeed a maximizer at $f(x^\\star, y)$.\n",
    "\n",
    "\n",
    "\n",
    "Now we can check $f(x, y^\\star)$ for any $x$.\n",
    "\n",
    "$$\n",
    "f(x, y^*) = f\\left( x, \\tfrac{c}{a} \\right)\n",
    "= (a x - b)^T (a \\tfrac{c}{a} - c) = 0\n",
    "$$\n",
    "\n",
    "Therefore we have that\n",
    "\n",
    "$$\n",
    "f(x^\\star, y^\\star) \\le f(x, y^\\star) \\qquad \\forall\\, x\n",
    "$$\n",
    "\n",
    "as $0 = 0$.\n",
    "\n",
    "So $x^\\star$ is indeed a minimizer of $f(x, y^\\star)$.\n",
    "\n",
    "\n",
    "For $(x^\\star, y^\\star)$ we indeed have\n",
    "\n",
    "$$\n",
    "f(x^\\star, y) \\le f(x^\\star, y^\\star) \\le f(x, y^\\star) \\qquad \\forall x, y\n",
    "$$\n",
    "\n",
    "as all terms are equal to zero.\n",
    "\n",
    "Therefore the solution to\n",
    "\n",
    "$$\n",
    "\\min_x \\max_y f(x,y)\n",
    "$$\n",
    "\n",
    "is\n",
    "\n",
    "$$\n",
    "(x^\\star, y^\\star) = \\left( \\tfrac{b}{a},\\ \\tfrac{c}{a} \\right).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(3)__ (15 points) One possible attempt at finding this solution via iterative\n",
    "        first-order methods is to perform gradient updates on the variables $x$\n",
    "        and $y$. More precisely for $\\gamma > 0$ consider the simultaneous gradient\n",
    "        descent/ascent updates\n",
    "        \n",
    "\\begin{equation}\n",
    "    x^{k+1} = x^k - \\gamma \\nabla_x f(x^k, y^k), \\qquad\n",
    "    y^{k+1} = y^k + \\gamma \\nabla_y f(x^k, y^k) \\nonumber\n",
    "\\end{equation}\n",
    "Show that the sequence of iterates $\\{x^k, y^k \\}_{k=0}^\\infty$ starting\n",
    "from any point $(x^0, y^0) \\neq (x^\\star, y^\\star)$ diverges, for any $\\gamma > 0$.\n",
    "Find the rate at which the distance from \n",
    "$(x^\\star,y^\\star)$ to the sequence $\\{x^k, y^k \\}$ grows as the number of iterations $k$ increases.\n",
    "\n",
    "**HINT:** Define $d_k^2=||(x^k,y^k)-(x^\\star, y^\\star)||_2^2$ as the sequence of squared distances to the optimum. If you find a formula for how $d_{k+1}$ depends on $d_k$ using the exact gradient updates for our $f$, you can easily argue for the divergence and the rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's expand $d_k^2$:\n",
    "\n",
    "$$\n",
    "d_k^2 = \\|(x^{k}, y^{k}) - (x^\\star, y^\\star)\\|_2^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\|(x^k - x^\\star) + (y^\\star - y^k)\\|_2^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\|x^k - x^\\star\\|_2^2 + \\|y^k - y^\\star\\|_2^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (x_1^k - x_1^\\star)^2 + (x_2^k - x_2^\\star)^2 \n",
    "  + (y_1^k - y_1^\\star)^2 + (y_2^k - y_2^\\star)^2\n",
    "$$\n",
    "\n",
    "Now let us calculate $d_{k+1}^2$.\n",
    "\n",
    "Recall:\n",
    "\n",
    "$$\n",
    "\\nabla_x f(x,y) = \n",
    "\\begin{pmatrix}\n",
    "a^2 y_1 - a c_1 \\\\\n",
    "a^2 y_2 - a c_2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_y f(x,y) = \n",
    "\\begin{pmatrix}\n",
    "a^2 x_1 - a b_1 \\\\\n",
    "a^2 x_2 - a b_2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "d_{k+1}^2 = \\|(x^{k} - y^{k+1}) - (x^\\star - y^\\star)\\|_2^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\|(x^k - \\gamma \\nabla_x f(x^k,y^k) - x^\\star),\n",
    "    (y^k - y^\\star + \\gamma \\nabla_y f(x^k,y^k))\\|_2^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\|x^k - x^\\star - \\gamma \\nabla_x f(x^k,y^k)\\|_2^2 \n",
    "  + \\|y^k - y^\\star + \\gamma \\nabla_y f(x^k,y^k)\\|_2^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (x_1^k - x_1^\\star - \\gamma(a^2 y_1^k - a c_1))^2\n",
    "+ (x_2^k - x_2^\\star - \\gamma(a^2 y_2^k - a c_2))^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\quad + (y_1^k - y_1^\\star + \\gamma(a^2 x_1^k - a b_1))^2\n",
    "+ (y_2^k - y_2^\\star + \\gamma(a^2 x_2^k - a b_2))^2\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For simplicity let's denote\n",
    "\n",
    "$$\n",
    "(x_1^k - x_1^\\star) = d_{x_1}^k, \\qquad\n",
    "(x_2^k - x_2^\\star) = d_{x_2}^k,\n",
    "$$\n",
    "\n",
    "$$\n",
    "(y_1^k - y_1^\\star) = d_{y_1}^k, \\qquad\n",
    "(y_2^k - y_2^\\star) = d_{y_2}^k\n",
    "$$\n",
    "\n",
    "Now we have that:\n",
    "\n",
    "$$\n",
    "d_k^2 = (d_{x_1}^k)^2 + (d_{x_2}^k)^2 + (d_{y_1}^k)^2 + (d_{y_2}^k)^2\n",
    "$$\n",
    "\n",
    "And:\n",
    "\n",
    "$$\n",
    "d_{k+1}^2 =\n",
    "(d_{x_1}^k - \\gamma(a^2 y_1^k - a c_1))^2\n",
    "+ (d_{x_2}^k - \\gamma(a^2 y_2^k - a c_2))^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\quad + (d_{y_1}^k + \\gamma(a^2 x_1^k - a b_1))^2\n",
    "+ (d_{y_2}^k + \\gamma(a^2 x_2^k - a b_2))^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (d_{x_1}^k)^2 + (d_{x_2}^k)^2 + (d_{y_1}^k)^2 + (d_{y_2}^k)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "+ \\gamma^2 \\left[(a^2 y_1^k - a c_1)^2 + (a^2 y_2^k - a c_2)^2\n",
    "+ (a^2 x_1^k - a b_1)^2 + (a^2 x_2^k - a b_2)^2\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "+ 2\\gamma\\left[d_{y_1}^k(a^2 x_1^k - a b_1)\n",
    "+ d_{y_2}^k(a^2 x_2^k - a b_2)\n",
    "- d_{x_1}^k(a^2 y_1^k - a c_1)\n",
    "- d_{x_2}^k(a^2 y_2^k - a c_2)\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= d_k^2  +\n",
    "$$\n",
    "$$\n",
    "+ \\gamma^2 \\left[(a^2 y_1^k - a c_1)^2 + (a^2 y_2^k - a c_2)^2\n",
    "+ (a^2 x_1^k - a b_1)^2 + (a^2 x_2^k - a b_2)^2\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "+ 2\\gamma\\left[d_{y_1}^k(a^2 x_1^k - a b_1)\n",
    "+ d_{y_2}^k(a^2 x_2^k - a b_2)\n",
    "- d_{x_1}^k(a^2 y_1^k - a c_1)\n",
    "- d_{x_2}^k(a^2 y_2^k - a c_2)\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Note that:\n",
    "\n",
    "$$\n",
    "x^\\star = \\frac{b}{a} = \\begin{pmatrix} b_1/a \\\\ b_2/a \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y^\\star = \\frac{c}{a} = \\begin{pmatrix} c_1/a \\\\ c_2/a \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "From there:\n",
    "\n",
    "$$\n",
    "a^2 x_1^k - a b_1 = a^2(x_1^k - x_1^\\star) = a^2 d_{x_1}^k\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^2 x_2^k - a b_2 = a^2(x_2^k - x_2^\\star) = a^2 d_{x_2}^k\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^2 y_1^k - a c_1 = a^2(y_1^k - y_1^\\star) = a^2 d_{y_1}^k\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^2 y_2^k - a c_2 = a^2(y_2^k - y_2^\\star) = a^2 d_{y_2}^k\n",
    "$$\n",
    "\n",
    "\n",
    "Now we have\n",
    "\n",
    "$$\n",
    "d_{k+1}^2\n",
    "= d_k^2\n",
    "+ \\gamma^2 a^4\\big[(d_{x_1}^k)^2 + (d_{x_2}^k)^2 + (d_{y_1}^k)^2 + (d_{y_2}^k)^2\\big]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\quad + 2\\gamma\\big(\n",
    "d_{y_1}^k \\cdot a^2 d_{x_1}^k\n",
    "+ d_{y_2}^k \\cdot a^2 d_{x_2}^k\n",
    "- d_{x_1}^k \\cdot a^2 d_{y_1}^k\n",
    "- d_{x_2}^k \\cdot a^2 d_{y_2}^k\n",
    "\\big) = \n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "= d_k^2\n",
    "+ \\gamma^2 a^4\\big[(d_{x_1}^k)^2 + (d_{x_2}^k)^2 + (d_{y_1}^k)^2 + (d_{y_2}^k)^2\\big] = \n",
    "$$\n",
    "\n",
    "$$\n",
    "= d_k^2 + a^4\\gamma^2 d_k^2\n",
    "= d_k^2(1 + a^4\\gamma^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we conclude that  \n",
    "\n",
    "$$\n",
    "d_{k+1}^2 = (1 + a^4\\gamma^2)\\, d_k^2 .\n",
    "$$\n",
    "\n",
    "Since $a^4\\gamma^2 \\ge 0$, we have $1 + a^4\\gamma^2 \\ge 1$, and if $a\\gamma \\neq 0$ (which is the case) then\n",
    "$1 + a^4\\gamma^2 > 1$. Therefore  \n",
    "\n",
    "$$\n",
    "d_{k+1}^2 = (1 + a^4\\gamma^2)\\, d_k^2 > d_k^2,\n",
    "$$\n",
    "\n",
    "so the squared distance from $(x^\\star, y^\\star)$ grows without bound and the sequence\n",
    "$\\{(x^k,y^k)\\}$ diverges.\n",
    "\n",
    "Since\n",
    "$$\n",
    "d_{k+1} = \\sqrt{1 + a^4\\gamma^2}\\, d_k ,\n",
    "$$\n",
    "the distance from $(x^\\star, y^\\star)$ grows at rate  \n",
    "\n",
    "$$\n",
    "\\sqrt{1 + a^4\\gamma^2},\n",
    "$$\n",
    "\n",
    "while the squared distance grows at rate  \n",
    "\n",
    "$$\n",
    "1 + a^4\\gamma^2.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(Optional $\\star$)__ A second attempt at finding the solution via _alternating_ gradient descent ascent on $x$\n",
    "        and $y$. More precisely for $\\gamma > 0$ consider the update\n",
    "        \n",
    "\\begin{equation}\n",
    "    x^{k+1} = x^k - \\gamma \\nabla_x f(x^k, y^k), \\qquad\n",
    "    y^{k+1} = y^k + \\gamma \\nabla_y f(x^{k+1}, y^k) \\nonumber\n",
    "\\end{equation}\n",
    "Show that the sequence of iterates $\\{x^k, y^k \\}_{k=0}^\\infty$ starting\n",
    "from any point $(x^0, y^0) \\neq (x^\\star, y^\\star)$ i) never converges, ii) but still remains bounded under certain stepsize conditions.\n",
    "\n",
    "\n",
    "Note: in this example, for simplicity just consider $f:\\mathbb{R}^2 \\to \\mathbb{R}$ with variables $x,y$, where\n",
    "$x, y \\in \\mathbb{R}$, the function has the form of\n",
    "$\n",
    "f(x,y) = (a x - b) (a y - c), \\quad a \\neq 0, \\; b,c \\in \\mathbb{R}.\n",
    "$\n",
    "\n",
    "\n",
    "**HINT**: Reduce the problem to studying a linear system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the function  \n",
    "\n",
    "$$\n",
    "f(x,y) = (ax - b)(ay - c) = a^2xy - acx - aby + bc.\n",
    "$$\n",
    "\n",
    "We have  \n",
    "\n",
    "$$\n",
    "\\partial_x f(x,y) = a^2 y - ac, \\qquad\n",
    "\\partial_y f(x,y) = a^2 x - ab.\n",
    "$$\n",
    "\n",
    "The saddle point is  \n",
    "\n",
    "$$\n",
    "(x^\\star, y^\\star) = \\left(\\frac{b}{a}, \\frac{c}{a}\\right).\n",
    "$$\n",
    "\n",
    "We define  \n",
    "\n",
    "$$\n",
    "d x = x - x^\\star, \\qquad d y = y - y^\\star.\n",
    "$$\n",
    "\n",
    "\n",
    "Using the alternating update we get  \n",
    "\n",
    "$$\n",
    "d x^{k+1} = x^{k+1} - x^\\star \n",
    "= x^k - \\gamma (a^2 y^k - ac) - x^\\star\n",
    "= d x^k - \\gamma a^2 d y^k,\n",
    "$$\n",
    "\n",
    "and  \n",
    "\n",
    "$$\n",
    "d y^{k+1} = y^{k+1} - y^\\star\n",
    "= y^k + \\gamma (a^2 x^{k+1} - ab) - y^\\star\n",
    "= d y^k + \\gamma a^2 d x^{k+1}.\n",
    "$$\n",
    "\n",
    "By plugging in $d x^{k+1}$ in $d y^{k+1}$ we get  \n",
    "\n",
    "$$\n",
    "d y^{k+1}\n",
    "= d y^k + \\gamma a^2 (d x^k - \\gamma a^2 d y^k)\n",
    "= (1 - a^4 \\gamma^2) d y^k + a^2 \\gamma d x^k.\n",
    "$$\n",
    "\n",
    "Hence we can write the system as  \n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "d x^{k+1} \\\\\n",
    "d y^{k+1}\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1 & -a^2 \\gamma \\\\\n",
    "a^2 \\gamma & 1 - a^4 \\gamma^2\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "d x^{k} \\\\\n",
    "d y^{k}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Let  \n",
    "\n",
    "$$\n",
    "M =\n",
    "\\begin{pmatrix}\n",
    "1 & -a^2 \\gamma \\\\\n",
    "a^2 \\gamma & 1 - a^4 \\gamma^2\n",
    "\\end{pmatrix}, \\qquad\n",
    "z^k =\n",
    "\\begin{pmatrix}\n",
    "d x^{k} \\\\\n",
    "d y^{k}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Then  \n",
    "\n",
    "$$\n",
    "z^{k+1} = M z^k.\n",
    "$$\n",
    "\n",
    "If it converges to some $z_c$ then it holds  \n",
    "\n",
    "$$\n",
    "z_c = M z_c,\n",
    "$$\n",
    "\n",
    "which means that $z_c$ is the eigenvector of $M$ with eigenvalue $1$.  \n",
    "Let us look at the eigenvalues of $M$.\n",
    "\n",
    "We can plug in the value $1$ in the characteristic equation. We compute  \n",
    "\n",
    "$$\n",
    "\\det(M - I)\n",
    "=\n",
    "\\begin{vmatrix}\n",
    "1-1 & -a^2\\gamma \\\\\n",
    "a^2\\gamma & 1 - a^4\\gamma^2 - 1\n",
    "\\end{vmatrix}\n",
    "=\n",
    "\\begin{vmatrix}\n",
    "0 & -a^2\\gamma \\\\\n",
    "a^2\\gamma & -a^4\\gamma^2\n",
    "\\end{vmatrix}\n",
    "= a^4\\gamma^2.\n",
    "$$\n",
    "\n",
    "For $a \\neq 0$ and $\\gamma > 0$ this cannot be $0$.  \n",
    "Therefore the only chance for the sequence to converge is to be $z_c = 0$.\n",
    "\n",
    "Now let us look at the behaviour of the sequence\n",
    "\n",
    "$$\n",
    "z^k = M^k z^0.\n",
    "$$\n",
    "\n",
    "The convergence happens only if all eigenvalues of $M$ satisfy $|\\lambda_i| < 1$.\n",
    "Boundedness if $|\\lambda_i| \\le 1$.\n",
    "Divergence if one of them is greater than $1$ in absolute value.\n",
    "\n",
    "We compute the characteristic equation\n",
    "\n",
    "$$\n",
    "\\det(M - \\lambda I) = 0,\n",
    "$$\n",
    " \n",
    "\n",
    "$$\n",
    "\\begin{vmatrix}\n",
    "1-\\lambda & -a^2\\gamma \\\\\n",
    "a^2\\gamma & 1 - a^4\\gamma^2 - \\lambda\n",
    "\\end{vmatrix}\n",
    "= 0.\n",
    "$$\n",
    " \n",
    "\n",
    "$$\n",
    "\\lambda^2 + \\lambda(a^4\\gamma^2 - 2) + 1 = 0.\n",
    "$$\n",
    "\n",
    "The discriminant is  \n",
    "\n",
    "$$\n",
    "B^2 - 4AC\n",
    "= (a^4\\gamma^2 - 2)^2 - 4\n",
    "= a^8\\gamma^4 - 4a^4\\gamma^2 + 4 - 4\n",
    "= a^4\\gamma^2(a^4\\gamma^2 - 4).\n",
    "$$\n",
    "\n",
    "If  \n",
    "\n",
    "$$\n",
    "a^4\\gamma^2(a^4\\gamma^2 - 4) < 0\n",
    "$$\n",
    "\n",
    "then  \n",
    "\n",
    "$$\n",
    "a^4\\gamma^2 < 4, \\qquad |a^2\\gamma| < 2,\n",
    "$$\n",
    "\n",
    "and $\\lambda_1$ and $\\lambda_2$ are complex conjugate.\n",
    "\n",
    "Plus we have  \n",
    "\n",
    "$$\n",
    "\\lambda_1\\lambda_2 = |M| = 1\n",
    "$$\n",
    "\n",
    "so  \n",
    "\n",
    "$$\n",
    "|\\lambda_1|^2 = 1 \\quad\\Rightarrow\\quad\n",
    "|\\lambda_1| = |\\lambda_2| = 1.\n",
    "$$\n",
    "\n",
    "Hence $|\\lambda_1| = |\\lambda_2| = 1 \\Rightarrow$ it doesn’t converge, but it is bounded.\n",
    "\n",
    "For $|a^2\\gamma| > 2$ the eigenvalues are real, and because  \n",
    "\n",
    "$$\n",
    "\\lambda_1\\lambda_2 = 1,\n",
    "$$\n",
    "\n",
    "one of them is bigger than $1$, so the sequence diverges.\n",
    "\n",
    "Thus for the boundedness we need  \n",
    "\n",
    "$$\n",
    "a^2\\gamma < 2\n",
    "\\qquad\\Rightarrow\\qquad\n",
    "\\gamma < \\frac{2}{a^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 A concrete example: Rock - Paper - Scissors - Lizard - Spock - 40 points\n",
    "\n",
    "To make your previous abstract analysis more concrete, we will now look at the iconic Rock-Paper-Scissors-Lizard-Spock game. Sheldon and Leonard always like to settle their differences through a round of this game, whose rules they state as follows:\n",
    "\n",
    "*\"Scissors cuts Paper, Paper covers Rock, Rock crushes Lizard, Lizard poisons Spock, Spock smashes Scissors, Scissors decapitates Lizard, Lizard eats Paper, Paper disproves Spock, Spock vaporizes Rock, (and as it always has) Rock crushes Scissors\"*\n",
    "\n",
    "Sheldon and Leonard always pick to play Spock because \"logic trumps all\", it will be your task to show that this is not the optimal strategy.\n",
    "\n",
    "We can formalize the game as follows. There are two players, the `x` player (e.g. Sheldon) and the `y` player (e.g. Leonard). There is only one single round. The players play a randomized strategy: each player chooses a probability of playing rock/paper/scissors/lizard/Spock. We look at the expected pay-off of these randomized strategies. \n",
    "\n",
    "In other words, the players choose an element in $\\Delta_5$ the probability simplex in dimension 5. The `x` player chooses a vector $\\mathbf{x} = \\begin{bmatrix} \\mathbf{x}_1 & \\mathbf{x}_2 & \\mathbf{x}_3 & \\mathbf{x}_4 & \\mathbf{x}_5 \\end{bmatrix} \\in \\Delta_5$ where $\\mathbf{x}_1$ is the probability of playing `Rock`, $\\mathbf{x}_2$ is the probability of playing `Paper`, $\\mathbf{x}_3$ is the probability of playing `Scissors`, $\\mathbf{x}_4$ is the probability of playing `Lizard`, $\\mathbf{x}_5$ is the probability of playing `Spock`. The `y` player chooses a vector $\\mathbf{y} \\in \\Delta_5$ defined in the same way. \n",
    "\n",
    "The game designers decide that winning the game gives 1 point and a tie gives 0 points. So the expected payoff for a give choice of strategies $\\mathbf{x}, \\mathbf{y}$ is obtained by computing:\n",
    "$$\n",
    "\\mathbf{x}^\\top \\begin{bmatrix} 0 & 1 & -1 & -1 & 1 \\\\ -1 & 0 & 1 & 1 & -1 \\\\ 1 & -1 & 0 & -1 & 1 \\\\ 1 & -1 & 1 & 0 & -1 \\\\ -1 & 1 & -1 & 1 & 0 \\end{bmatrix}\\mathbf{y}\n",
    "$$\n",
    "\n",
    "The `x` player wants to minimize this expected payoff and the `y` player wants to maximize it. So the problem we seek to solve is\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{x} \\in \\Delta_5} \\max_{\\mathbf{y} \\in \\Delta_5} \\mathbf{x}^\\top \\mathbf{M}\\mathbf{y} =: f(\\mathbf{x}, \\mathbf{y})\n",
    "$$\n",
    "with $\\mathbf{M} = \\begin{bmatrix} 0 & 1 & -1 & -1 & 1 \\\\ -1 & 0 & 1 & 1 & -1 \\\\ 1 & -1 & 0 & -1 & 1 \\\\ 1 & -1 & 1 & 0 & -1 \\\\ -1 & 1 & -1 & 1 & 0 \\end{bmatrix}$.\n",
    "\n",
    "In the following cells, you will implement methods to solve this game and find the optimal strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ipywidgets import interact, SelectionSlider, fixed\n",
    "from itertools import combinations\n",
    "from scipy.stats import entropy\n",
    "from lib.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** (2 points) Implement the objective function $f$ given two 3 dimensional vectors `x` and `y` stored as `torch.Tensor` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = ???\n",
    "def f(x: torch.Tensor, y: torch.Tensor):\n",
    "    return ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** (10 points) Implement a single step of the simultaneous gradient descent/ascent studied in question 1.1.(3). Since we are in a constrained setting, make sure you include a projection step onto the simplex. \n",
    "\n",
    "The function `GDA` you will implement takes in the objective function `f`, the two current strategies of the players stored in tensors `x` and `y` and a step_size. Write the function so that it modifies the variables `x` and `y` in place without returning anything.\n",
    "\n",
    "We provide you with a function called `simplex_project` that projects a vector on the probability simplex. The function has no return value and does the projection in place. Use `Pytorch` to compute gradients automatically. \n",
    "\n",
    "__Hints__: Review what a call to `.backward()` on a tensor does. Review what in-place operations are like `.add_`. Think of which steps need to be in a `with torch.no_grad()` block. Remember to zero the gradients before re-using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GDA(f, x, y, step_size):\n",
    "    payoff = f(x, y)\n",
    "    ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run this algorithm initialized from Sheldon and Leonard's strategies of always playing Spock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init = torch.tensor([0.0, 0.0, 0.0, 0.0, 1.0])\n",
    "y_init = torch.tensor([0.0, 0.0, 0.0, 0.0, 1.0])\n",
    "\n",
    "gda_x_sequence, gda_y_sequence = run_alg(GDA, f, x_init, y_init, step_size=0.05, n_iterations=2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize this sequence as ${4 \\choose 2} = 6$ two dimensional slices since the simplex $\\Delta_5$ is 4-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_pairs = list(combinations(range(4), 2))\n",
    "def visualize_seq_slice(dim_pair):\n",
    "    visualize_seq(gda_x_sequence[:, dim_pairs[dim_pair]], \n",
    "                  gda_y_sequence[:, dim_pairs[dim_pair]],\n",
    "                  dim_pairs[dim_pair])\n",
    "interact(visualize_seq_slice, dim_pair=SelectionSlider(\n",
    "    options=range(len(dim_pairs)),\n",
    "    value=0,\n",
    "    description=\"Dim pair:\",\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have plots that aggregate the convergence status over all 5 dimensions at the same time, let's also visualize:\n",
    "\n",
    "- the duality gap over time: $g_k = \\max_{\\mathbf{y} \\in \\Delta_5}{f(\\mathbf{x}_k,\\mathbf{y})} - \\min_{\\mathbf{x} \\in \\Delta_5}{f(\\mathbf{x},\\mathbf{y}_k)} = \\max_{i}{(\\mathbf{M}^\\top \\mathbf{x}_k)_i} - \\min_{i}{(\\mathbf{M} \\mathbf{y}_k)_i}$\n",
    "- the distance to the optimum over time: $d_k=\\sqrt{||\\mathbf{x}_k-\\mathbf{x}^\\star||_2^2 + ||\\mathbf{y}_k-\\mathbf{y}^\\star||_2^2}$\n",
    "- the discrete entropy over time of the x and y strategies (we can do this since they are probability distributions): $H(\\mathbf{x}_k)=-\\sum_{i=1}^{5}{\\mathbf{x}_k^{(i)}\\log{\\mathbf{x}_k^{(i)}}}, H(\\mathbf{y}_k)=-\\sum_{i=1}^{5}{\\mathbf{y}_k^{(i)}\\log{\\mathbf{y}_k^{(i)}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_opt(L_x, L_y):\n",
    "    return np.sqrt(np.sum((L_x - 0.2) ** 2 + (L_y - 0.2) ** 2, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duality_gap(L_x, L_y):\n",
    "    return (np.max(M.numpy().T.reshape(1, 5, 5) @ L_x.reshape(-1, 5, 1), axis=(1, 2)) \n",
    "            - np.min(M.numpy().reshape(1, 5, 5) @ L_y.reshape(-1, 5, 1), axis=(1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-poster')\n",
    "fig, ax = plt.subplots(3, 1, figsize=(11.7, 1.5 * 8.3))\n",
    "ax[0].plot(range(len(gda_x_sequence)), entropy(gda_x_sequence, axis=1), lw=5, color='b', label=\"GDA x\")\n",
    "ax[0].plot(range(len(gda_y_sequence)), entropy(gda_y_sequence, axis=1), lw=5, color='r', label=\"GDA y\")\n",
    "ax[0].axline((0, np.log(5)), slope=0, color='black', lw=5, label=\"Max\")\n",
    "ax[0].legend()\n",
    "ax[0].set_xscale(\"log\")\n",
    "ax[0].set_xlabel(\"k\")\n",
    "ax[0].set_ylabel(\"Entropy\")\n",
    "ax[1].plot(range(len(gda_x_sequence)), distance_to_opt(gda_x_sequence, gda_y_sequence), lw=5, label=\"GDA\")\n",
    "ax[1].legend()\n",
    "ax[1].set_yscale(\"log\")\n",
    "ax[1].set_xlabel(\"k\")\n",
    "ax[1].set_ylabel(\"$d_k$\")\n",
    "ax[2].plot(range(len(gda_x_sequence)), duality_gap(gda_x_sequence, gda_y_sequence), lw=5, label=\"GDA\")\n",
    "ax[2].legend()\n",
    "ax[2].set_yscale(\"log\")\n",
    "ax[2].set_xlabel(\"k\")\n",
    "ax[2].set_ylabel(\"$g_k$\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)** (3 point) What kind of behavior do you observe ? Do the iterates converge ? Play with the step_size and the number of iterations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4)** (4 points) As you've shown both theoretically and in practice, simultaneous Gradient Descent Ascent (sGDA) cannot solve the problem. You will now implement algorithm that can solve bilinear games. We will denote the projection onto the decision space of the x-player and y-player, $\\Pi_{\\mathcal X}$ and $\\Pi_{\\mathcal Y}$, respectively.\n",
    "\n",
    "\n",
    "Let $\\mathbf{z}_k = \\begin{bmatrix} x_k \\\\ y_k \\end{bmatrix}$, $\\Pi(\\mathbf{z})=\\begin{bmatrix} \\Pi_{\\mathcal X}(x) \\\\ \\Pi_{\\mathcal Y}(y) \\end{bmatrix}$ and $G(\\mathbf{z}_k) = \\begin{bmatrix} \\nabla_x f(x_k,y_k) \\\\ -\\nabla_y f(x_k,y_k) \\end{bmatrix}$.\n",
    "\n",
    "Consider the following implicit updates, which is the `Proximal Point Method ` (PPM) for solving the bilinear games:\n",
    "\\begin{equation}\n",
    "    \\mathbf{z}_{k+1} = \\Pi(\\mathbf{z}_k - \\gamma G(\\color{red}{\\mathbf{z}_{k+1}})) \\nonumber\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s first prove that the Proximal Point Method (PPM) converges in a simple case to build intuition. Consider the aforementioned stylized function $f: \\mathbb{R}^2 \\to \\mathbb{R}$, such that $f(x, y)=xy$, and consider simplified unconstrained probelm, i.e., the projection is the identity map. Prove that PPM converges.\n",
    "\n",
    "[Hint] Rewrite the implicit update as an explicit update in the form of $z_{k+1} = A z_{k}$ for some matrix $A$.\n",
    "\n",
    "[Hint] A square matrix $A$ satisfies $\\lim_{k \\to \\infty} A^k = 0$ if its spectral radius $\\rho(A) < 1$, where the spectral radius is defined by\n",
    "$\n",
    "\\rho(A) := \\max_i |\\lambda_i|,\n",
    "$\n",
    "with $\\{\\lambda_i\\}$ denoting the  eigenvalues of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5)** (6 points)  In general, computing the next iterate $\\mathbf{z}_{k+1}$ in PPM requires you to solve a fixed point problem because we are evaluating the gradient at the unknown next iterate. As this is too costly, we circumvent this difficulty by doing an _extrapolation_ step. The idea behind `ExtraGradient` (EG) is to approximate an implicit update with a more tractable one. Define the half steps:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{z}_{k+1/2} = \\Pi(\\mathbf{z}_k - \\gamma G(\\mathbf{z}_k)) \\nonumber\n",
    "\\end{equation}\n",
    "\n",
    "These half step extrapolation variables will help us to approximate the implicit iterates. We can then write\n",
    "\\begin{equation}\n",
    "    \\mathbf{z}_{k+1} = \\Pi(\\mathbf{z}_k - \\gamma G(\\color{green}{\\mathbf{z}_{k+1/2}})) \\nonumber\n",
    "\\end{equation}\n",
    "The recursion above defines the ExtraGradient algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtraGradient(f, x, y, step_size):\n",
    "    payoff = f(x, y)\n",
    "    ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_x_sequence, eg_y_sequence = run_alg(ExtraGradient, f, x_init, y_init, step_size=0.1, n_iterations=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_pairs = list(combinations(range(4), 2))\n",
    "def visualize_seq_slice(dim_pair):\n",
    "    visualize_seq(eg_x_sequence[:, dim_pairs[dim_pair]], \n",
    "                  eg_y_sequence[:, dim_pairs[dim_pair]],\n",
    "                  dim_pairs[dim_pair])\n",
    "interact(visualize_seq_slice, dim_pair=SelectionSlider(\n",
    "    options=range(len(dim_pairs)),\n",
    "    value=0,\n",
    "    description=\"Dim pair:\",\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-poster')\n",
    "fig, ax = plt.subplots(3, 1, figsize=(11.7, 1.5 * 8.3))\n",
    "ax[0].plot(range(len(gda_x_sequence)), entropy(gda_x_sequence, axis=1), lw=5, color='b', alpha=0.1, label=\"GDA x\")\n",
    "ax[0].plot(range(len(gda_y_sequence)), entropy(gda_y_sequence, axis=1), lw=5, color='r', alpha=0.1, label=\"GDA y\")\n",
    "ax[0].plot(range(len(eg_x_sequence)), entropy(eg_x_sequence, axis=1), lw=5, color='b', label=\"EG x\")\n",
    "ax[0].plot(range(len(eg_y_sequence)), entropy(eg_y_sequence, axis=1), lw=5, color='r', label=\"EG y\")\n",
    "ax[0].axline((0, np.log(5)), slope=0, color='black', lw=5, alpha=0.5, label=\"Max\")\n",
    "ax[0].legend()\n",
    "ax[0].set_xscale(\"log\")\n",
    "ax[0].set_xlabel(\"k\")\n",
    "ax[0].set_ylabel(\"Entropy\")\n",
    "ax[1].plot(range(len(gda_x_sequence)), distance_to_opt(gda_x_sequence, gda_y_sequence), lw=5, label=\"GDA\")\n",
    "ax[1].plot(range(len(eg_x_sequence)), distance_to_opt(eg_x_sequence, eg_y_sequence), lw=5, label=\"EG\")\n",
    "ax[1].legend()\n",
    "ax[1].set_yscale(\"log\")\n",
    "ax[1].set_xlabel(\"k\")\n",
    "ax[1].set_ylabel(\"$d_k$\")\n",
    "ax[2].plot(range(len(gda_x_sequence)), duality_gap(gda_x_sequence, gda_y_sequence), lw=5, label=\"GDA\")\n",
    "ax[2].plot(range(len(eg_x_sequence)), duality_gap(eg_x_sequence, eg_y_sequence), lw=5, label=\"EG\")\n",
    "ax[2].legend()\n",
    "ax[2].set_yscale(\"log\")\n",
    "ax[2].set_xlabel(\"k\")\n",
    "ax[2].set_ylabel(\"$g_k$\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(BONUS)** What can you observe about the optimal solution? What properties does the optimal Rock-Paper-Scissors-Lizard-Spock strategy have? Prove that extra-gradient in the bilinear case doesn't diverge like GDA. (Again, candy reward for correct answers!)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(6)** (10 points) It turns out that we can _generalize_ the ExtraGradient method naturally by striving for a bit more accurate extrapolation. Namely, instead of always performing 1 extrapolation step updating half-iterates before the main parameter update, we can perform $m\\geq 1$ fractional steps. Implement this ClairvoyantExtraGradient (CEG) method [1], with the precise update step formulas given below:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{z}_{k+1(\\color{green}{1/m+1})} = \\Pi(\\mathbf{z}_k - \\gamma G(\\color{green}{\\mathbf{z}_{k}})) \\\\\n",
    "    \\mathbf{z}_{k+1(\\color{green}{2/m+1})} = \\Pi(\\mathbf{z}_{k+1(\\color{green}{1/m+1})} - \\gamma G(\\mathbf{z}_{k+1(\\color{green}{1/m+1})})) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{z}_{k+1(\\color{green}{m/m+1})} = \\Pi(\\mathbf{z}_{k+1(\\color{green}{m-1/m+1})} - \\gamma G(\\mathbf{z}_{k+1(\\color{green}{m-1/m+1})})) \\\\\n",
    "    \\mathbf{z}_{k+1} = \\Pi(\\mathbf{z}_k - \\gamma G({\\mathbf{z}_{k+1(\\color{green}{m/m+1})} })) \\nonumber\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "[1] Cevher, Volkan, Georgios Piliouras, Ryann Sim, and Stratis Skoulakis. “Min-Max Optimization Made Simple: Approximating the Proximal Point Method via Contraction Maps.” In 2023 Symposium on Simplicity in Algorithms (SOSA), 192–206. Proceedings. Society for Industrial and Applied Mathematics, 2023. https://doi.org/10.1137/1.9781611977585.ch18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClairvoyantExtraGradient(f, x, y, step_size, m=1):\n",
    "    payoff = f(x, y)\n",
    "    ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_values = [1, 2, 3, 5, 10, 25] # You can change this if you wish\n",
    "ceg_x_sequences, ceg_y_sequences = [], []\n",
    "for m in m_values:\n",
    "    print(\"m =\", m)\n",
    "    step_size = 0.1 if m != 10 else 0.09 # You can change this if you wish\n",
    "    ceg_x_sequence, ceg_y_sequence = run_alg(ClairvoyantExtraGradient, f, x_init, y_init, \n",
    "                                             n_iterations=2500, step_size=step_size, m=m)\n",
    "    ceg_x_sequences.append(ceg_x_sequence)\n",
    "    ceg_y_sequences.append(ceg_y_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_pairs = list(combinations(range(4), 2))\n",
    "def visualize_seq_slice(L_x, L_y, dim_pair):\n",
    "    visualize_seq(L_x[:, dim_pairs[dim_pair]], \n",
    "                  L_y[:, dim_pairs[dim_pair]],\n",
    "                  dim_pairs[dim_pair])\n",
    "for m, ceg_x_sequence, ceg_y_sequence in zip(m_values, ceg_x_sequences, ceg_y_sequences):\n",
    "    print(\"m =\", m)\n",
    "    interact(visualize_seq_slice, L_x=fixed(ceg_x_sequence), L_y=fixed(ceg_y_sequence), dim_pair=SelectionSlider(\n",
    "        options=range(len(dim_pairs)),\n",
    "        value=0,\n",
    "        description=\"Dim pair:\",\n",
    "        disabled=False,\n",
    "        continuous_update=False,\n",
    "        orientation=\"horizontal\",\n",
    "        readout=True\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-poster')\n",
    "fig, ax = plt.subplots(3, 1, figsize=(11.7, 1.5 * 8.3))\n",
    "ax[0].plot(range(len(gda_x_sequence)), entropy(gda_x_sequence, axis=1), lw=5, alpha=0.1, label=\"GDA x\")\n",
    "ax[1].plot(range(len(gda_x_sequence)), distance_to_opt(gda_x_sequence, gda_y_sequence), lw=5, label=\"GDA\")\n",
    "ax[2].plot(range(len(gda_x_sequence)), duality_gap(gda_x_sequence, gda_y_sequence), lw=5, label=\"GDA\")\n",
    "for m, ceg_x_sequence, ceg_y_sequence in zip(m_values, ceg_x_sequences, ceg_y_sequences):\n",
    "    ax[0].plot(range(len(ceg_x_sequence)), entropy(ceg_x_sequence, axis=1), lw=5, label=f\"CEG-{m} x\")\n",
    "    ax[1].plot(range(len(ceg_x_sequence)), distance_to_opt(ceg_x_sequence, ceg_y_sequence), lw=5, label=f\"CEG-{m}\")\n",
    "    ax[2].plot(range(len(ceg_x_sequence)), duality_gap(ceg_x_sequence, ceg_y_sequence), lw=5, label=f\"CEG-{m}\")\n",
    "ax[0].axline((0, np.log(5)), slope=0, color='black', lw=5, alpha=0.5, label=\"Max\")\n",
    "ax[0].legend()\n",
    "ax[0].set_xscale(\"log\")\n",
    "ax[0].set_xlabel(\"k\")\n",
    "ax[0].set_ylabel(\"Entropy\")\n",
    "ax[1].legend()\n",
    "ax[1].set_yscale(\"log\")\n",
    "ax[1].set_xlabel(\"k\")\n",
    "ax[1].set_ylabel(\"$d_k$\")\n",
    "ax[2].legend()\n",
    "ax[2].set_yscale(\"log\")\n",
    "ax[2].set_xlabel(\"k\")\n",
    "ax[2].set_ylabel(\"$g_k$\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(7)** (5 points) Discuss what you observe for the CEG runs. How do the value of $m$ and the step size influence the convergence and the computation cost? Is there a sweet spot?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "py:light,ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
